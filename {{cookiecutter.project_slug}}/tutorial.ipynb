{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning using ART\n",
    "\n",
    "Hi! In this tutorial, we will walk you through the process of using ART to perform transfer learning. We will use the [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) dataset and the `bert-base-cased` model from HuggingFace. We will train a classifier to predict the sentiment of a review (positive or negative) and then we will use ART to perform transfer learning to attack the classifier. Most of the code will follow [HF's tutorial](https://huggingface.co/docs/transformers/training) with some modifications to make it work with ART.\n",
    "\n",
    "Just to remind you - the main goal of ART is to follow [Karpathys' recipe of training neural networks](https://karpathy.github.io/2019/04/25/recipe/).\n",
    "\n",
    "We'll do everything in a script, your task will be to fill the `run.py` accordingly with our instructions from this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install art nltk wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we need to download the data and do some analysis on it. We'll use the `datasets` library from HuggingFace to do this, and we'll wrap the model to Lightning's `DataModule` to make it easier to use with PyTorch Lightning. We prepared the dataset for you in `dataset.py`, check it out there - it's nothing more than just a simple Lightning's Datamodule. The main function in `run.py` is just ready to download the data and show you a sample from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Correct main()</summary>\n",
    "\n",
    "```py\n",
    "\n",
    "data = YelpReviews()\n",
    "print(data.dataset[\"train\"][100])\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can become one with the data. We want to know some statistics, that will be helpful throughout the whole project. We prepared for you a data analysis step in `steps.py`. \n",
    "Now it's your turn! Fill the `...` places in `steps.py` to get the data statistics. You can use `dataset.py` to get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hint for filling TextDataAnalisys</summary>\n",
    "\n",
    "* For `number_of_classes` you can count the unique targets in the dataset\n",
    "* For class_names you can get names from targets as well\n",
    "* For class counts just sum all examples for each class form targets\n",
    "* For number_of_unique_words you need to calculate the number of unique words in the dataset.\n",
    "* You can make use of `Counter` from `collections` library and `np.unique` from `numpy` library\n",
    "\n",
    "* Notice the `log_params` function - it's an important function that will log specigied by you parameters to the logger. You can use it to log the results of your analysis.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Correct TextDataAnalisys</summary>\n",
    "\n",
    "```python\n",
    "    def do(self, previous_states):\n",
    "        targets = []\n",
    "        texts = []\n",
    "\n",
    "        # Loop through batches in the YelpReviews datamodule train dataloader\n",
    "        for batch in self.datamodule.train_dataloader():\n",
    "            # Assuming 'labels' contains the review scores\n",
    "            targets.extend(batch['label'])\n",
    "            # Assuming 'text' contains the review text\n",
    "            texts.extend(batch['text'])\n",
    "\n",
    "        # Calculate the number of unique classes (review scores) in the targets\n",
    "        number_of_classes = len(np.unique(targets))\n",
    "\n",
    "        # Now tell me what the scores are\n",
    "        class_names = [str(i) for i in sorted(np.unique(targets))]\n",
    "\n",
    "        # Create a dictionary of class names and their counts\n",
    "        targets_ints = [int(i) for i in targets]\n",
    "        class_counts = Counter(targets_ints)\n",
    "\n",
    "        # count number of unique words\n",
    "        unique_words = set()\n",
    "        for text in texts:\n",
    "            unique_words.update(text.split())\n",
    "        number_of_unique_words = len(unique_words)\n",
    "\n",
    "        # Create a word cloud\n",
    "        wordcloud = WordCloud().generate(' '.join(texts))\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        MatplotLibSaver().save(\n",
    "            fig, self.get_step_id(), self.name, \"wordcloud\"\n",
    "        )\n",
    "\n",
    "        self.results.update(\n",
    "            {\n",
    "                \"number_of_classes\": number_of_classes,\n",
    "                \"class_names\": class_names,\n",
    "                \"number_of_reviews_in_each_class\": class_counts,\n",
    "            }\n",
    "        )\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've done it, modify the main() function as follows:\n",
    "* read the data\n",
    "* start the ART project\n",
    "* add our data analysis step with checking, whether the result exists\n",
    "* run all the steps (for now we have just one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hint for filling main()</summary>\n",
    "\n",
    "* To read the data just initialize the `YelpReviews` class from `dataset.py` - as we did in the previous step.\n",
    "* To start the ART project you need to initialize the ArtProject class from art.project. To do it you just need a name of your project (let it be \"yelpreviews\") and the data, you've already read\n",
    "* To add the step you need to call `add_step` method from ArtProject class. You need to pass the step class (in this case it's `TextDataAnalysis` from `steps.py` you've just filled) and a list of checks to perform. In our case we want to check whether the result from our step exist, so we use `CheckResultExists` class from `art.checks` for each of [\"number_of_classes\", \"class_names\", \"number_of_reviews_in_each_class\", \"number_of_unique_words\"]\n",
    "* To run the project you need to call `run_all` method from the already initialized project. It will run all the steps you've added to the project.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Correct main()</summary>\n",
    "\n",
    "```py\n",
    "def main():\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "    project.add_step(\n",
    "        TextDataAnalysis(),\n",
    "        [\n",
    "            CheckResultExists(\"number_of_classes\"),\n",
    "            CheckResultExists(\"class_names\"),\n",
    "            CheckResultExists(\"number_of_reviews_in_each_class\"),\n",
    "            CheckResultExists(\"number_of_unique_words\")\n",
    "        ])\n",
    "    project.run_all()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can see the output below, and the wordcloud.png in checkpoints folder we're good to go!\n",
    "```\n",
    "Steps status:\n",
    "data_analysis_Data analysis: Completed. Results:\n",
    "        number_of_classes: 5\n",
    "        class_names: ['0', '1', '2', '3', '4']\n",
    "        number_of_reviews_in_each_class: Counter({1: 240, 2: 208, 4: 189, 0: 189, 3: 174})```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra tasks**\n",
    "* Try to write your check to check whether the wordcloud exists\n",
    "* Try to calculate more statistics that you find useful, save them in the results, and add checks in the `run.py` to verify whether they exist!\n",
    "* Try to log the results in log_params function in `steps.py`\n",
    "* Try to check whether the number of unique words is greater than 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of metrics in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have the data, we ca work on our models, which will solve the sentiment analysis problem! We start with a simple baseline. But before that, we need to define metrics that we'll use throughout the entire experiment:\n",
    "\n",
    "* Calculate the number of classes - you can write it by yourself or use the `number_of_classes` from the results of the previous step\n",
    "* Define metrics - we'll use Accuracy, Precision, Recall, and the CrossEntropyLoss. Initialize each of them in a list `METRICS`\n",
    "* pass that list to the project - `project.register_metrics(METRICS)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Correct main()</summary>\n",
    "\n",
    "```py\n",
    "def main():\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(TextDataAnalysis(), [\n",
    "                     CheckResultExists(\"number_of_classes\"),\n",
    "                     CheckResultExists(\"class_names\"),\n",
    "                     CheckResultExists(\"number_of_reviews_in_each_class\"),])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you should see, that the first step was skipped, because we already have executed it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But why do we need metrics defined for the project?**\n",
    "\n",
    "Take a look at the `MetricsCalculator` from the ART. It takes care of calculating metrics for each step in the project. It's a very useful class, as it allows us to calculate metrics for each step in the project, and then we can use them to compare different models. It's also very useful when we want to compare different models on the same dataset. We can just add the metrics to the project, and then we can compare them in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every project, we have to start from the baselines! We prepared one baseline for you in `models/simple_baseline.py`. \n",
    "\n",
    "The baseline, as every other model used in ART, has to inherit from the `ArtModule` which is a wrapper for PyTorch Lightning's `LightningModule`. The `ArtModule` has a few useful methods, that we'll use in our project. The most important is the integration with the previously mentioned `MetricsCalculator`, but we'll come to that later when we develop the first deep learning model. For now, we use `ml_parse_data` which parses data specifically for the non-deep-learning training (we don't use PyTorch there), and the `baseline_train` method, which \"trains\" the model. In our case, it's just calculating probabilities for each class and returning them. We'll use it to compare it with our deep learning models. Take attention to the `ml_parse_data` return format - it's a dictionary `{INPUT: X, TARGET: y}`\n",
    "\n",
    "Add the baseline to the project and run it:\n",
    "\n",
    "* Create a baseline callable object - do not initialize it!\n",
    "* Add the `EvaluateBaseline` step to the project by checking whether scores for each metric exist\n",
    "* Run the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hints for evaluating the baseline</summary>\n",
    "\n",
    "* To register the step call the `add_step` function with `step=EvaluateBaseline` from `art.steps` and `checks=[CheckResultExists(metric) for metric in METRICS]`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Correct main()</summary>\n",
    "\n",
    "```py\n",
    "def main():\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(TextDataAnalysis(), [\n",
    "                     CheckResultExists(\"number_of_classes\"),\n",
    "                     CheckResultExists(\"class_names\"),\n",
    "                     CheckResultExists(\"number_of_reviews_in_each_class\"),])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    baseline = HeuristicBaseline\n",
    "    project.add_step(\n",
    "        step=EvaluateBaseline(baseline),\n",
    "        checks=[CheckScoreExists(metric=METRICS[i])\n",
    "                for i in range(len(METRICS))],\n",
    "    )\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct output should look like this:\n",
    "```\n",
    "Steps status:\n",
    "\n",
    "data_analysis_Data analysis: Skipped. Results:\n",
    "        number_of_classes: 5\n",
    "        class_names: ['0', '1', '2', '3', '4']\n",
    "        number_of_reviews_in_each_class: {'4': 189, '1': 240, '3': 174, '0': 189, '2': 208}\n",
    "\n",
    "        \n",
    "HeuristicBaseline_2_Evaluate Baseline: Completed. Results:\n",
    "        MulticlassAccuracy-HeuristicBaseline-validate-Evaluate Baseline: 0.30702152848243713\n",
    "        MulticlassPrecision-HeuristicBaseline-validate-Evaluate Baseline: 0.3316725790500641\n",
    "        MulticlassRecall-HeuristicBaseline-validate-Evaluate Baseline: 0.30702152848243713\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra tasks**\n",
    "* Try to write your own baseline in `models/baseline2.py` and evaluate it in the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the proper model\n",
    "\n",
    "As you might already know from the choice of tokenizer, we chose the bert-tiny for this problem. This dataset is hard, so we'll be able to obtain ~45% accuracy on the test set.\n",
    "\n",
    "We prepared the model for you in `models/bert.py`. It's a simple model, that uses the `prajjwal1/bert-tiny` model from HuggingFace. We use the `AutoModelForSequenceClassification` model, which is a model that takes a sequence of tokens and returns the logits for each class. We use the `AutoTokenizer` to tokenize the text, and then we use the `BertForSequenceClassification` to get the logits. We use the `AutoModelForSequenceClassification` with the `prajjwal1/bert-tiny` model because it's already trained on the sentiment analysis task, so we can use it as a starting point for our model. We could train the last layer of the model, which is a linear layer, and we'll be able to get some good results. But we'll also try to perform fine-tuning of the whole model, to see if we can get better results.\n",
    "\n",
    "Notice a few things:\n",
    "* We use the `ArtModule` as a wrapper for the `LightningModule` - it's a very useful class, as it allows us to use the `MetricsCalculator` to calculate metrics for each step in the project\n",
    "* Notice the `compute_loss()` - it only takes calculated loss from the `MetricsCalculator` which is passed inside the data dictionary. Pure ART's magic!\n",
    "* Pay attention to the format of returning predictions and data, as previously done in the baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the final model we'll perform some experiments:\n",
    "* Check loss on initialization - add `CheckLossOnInit` to the project\n",
    "* Overfitting one batch with an unfrozen backbone - add `OverfitOneBatch` to the project\n",
    "* Overfitting the entire dataset with an unfrozen backbone - add `OverfitEntireDataset` to the project\n",
    "\n",
    "\n",
    "Then, if our steps succeed we can perform training on the entire dataset - first with a frozen backbone, then with an unfrozen backbone and reduced learning rate - just add `TransferLearning` to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hints for checking loss on initialization</summary>\n",
    "\n",
    "* Register the model without initializing it - `model = YelpReviewsModel`\n",
    "* Try to calculate the expected loss of cross entropy for the number of classes you have in the dataset\n",
    "* Use logarithm to log the expected loss\n",
    "* Use `CheckLossOnInit` from `art.checks` to check whether the loss is correct\n",
    "* Initialize the `CheckLossOnInit` with `model=model` and one check in the list\n",
    "* Use CheckScoreCloseTo - the cross entropy loss defined in METRICS should be close to the expected loss\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>CheckLossOnInit in main()<b></summary>\n",
    "\n",
    "```py\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(\n",
    "        TextDataAnalysis(),\n",
    "        [\n",
    "            CheckResultExists(\"number_of_classes\"),\n",
    "            CheckResultExists(\"class_names\"),\n",
    "            CheckResultExists(\"number_of_reviews_in_each_class\"),\n",
    "            CheckResultExists(\"number_of_unique_words\")\n",
    "        ])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    baseline = HeuristicBaseline\n",
    "    project.add_step(\n",
    "        step=EvaluateBaseline(baseline),\n",
    "        checks=[CheckScoreExists(metric=METRICS[i])\n",
    "                for i in range(len(METRICS))],\n",
    "    )\n",
    "\n",
    "    model = YelpReviewsModel\n",
    "\n",
    "    EXPECTED_LOSS = - math.log(1 / NUM_CLASSES)\n",
    "    EXPECTED_LOSS\n",
    "    project.add_step(\n",
    "        CheckLossOnInit(model),\n",
    "        [CheckScoreCloseTo(metric=METRICS[3],\n",
    "                           value=EXPECTED_LOSS, rel_tol=0.1)]\n",
    "    )\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hints for overfitting one batch</summary>\n",
    "\n",
    "* Use `OverfitOneBatch` step, specify the number of epochs to 40-50\n",
    "* Use `CheckScoreLessThan` to check whether the loss less them some value you specify\n",
    "* The value could be e.g. `0.05`, but everyone can define the overfitting differently\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>OverfitOneBatch main()<b></summary>\n",
    "\n",
    "```py\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(\n",
    "        TextDataAnalysis(),\n",
    "        [\n",
    "            CheckResultExists(\"number_of_classes\"),\n",
    "            CheckResultExists(\"class_names\"),\n",
    "            CheckResultExists(\"number_of_reviews_in_each_class\"),\n",
    "            CheckResultExists(\"number_of_unique_words\")\n",
    "        ])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    baseline = HeuristicBaseline\n",
    "    project.add_step(\n",
    "        step=EvaluateBaseline(baseline),\n",
    "        checks=[CheckScoreExists(metric=METRICS[i])\n",
    "                for i in range(len(METRICS))],\n",
    "    )\n",
    "\n",
    "    model = YelpReviewsModel\n",
    "\n",
    "    EXPECTED_LOSS = - math.log(1 / NUM_CLASSES)\n",
    "    EXPECTED_LOSS\n",
    "    project.add_step(\n",
    "        CheckLossOnInit(model),\n",
    "        [CheckScoreCloseTo(metric=METRICS[3],\n",
    "                           value=EXPECTED_LOSS, rel_tol=0.1)]\n",
    "    )\n",
    "\n",
    "    project.add_step(\n",
    "        step=OverfitOneBatch(model, number_of_steps=40),\n",
    "        checks=[CheckScoreLessThan(metric=METRICS[3], value=0.05)],\n",
    "    )\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hints for overfitting the entire trainset</summary>\n",
    "\n",
    "* Use `Overfit` step, specify the number of epochs to 40-50\n",
    "* Use `CheckScoreLessThan` to check whether the loss less them some value you specify\n",
    "* The value could be e.g. `0.05`, but everyone can define the overfitting differently\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Overfit main()<b></summary>\n",
    "\n",
    "```py\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(\n",
    "        TextDataAnalysis(),\n",
    "        [\n",
    "            CheckResultExists(\"number_of_classes\"),\n",
    "            CheckResultExists(\"class_names\"),\n",
    "            CheckResultExists(\"number_of_reviews_in_each_class\"),\n",
    "            CheckResultExists(\"number_of_unique_words\")\n",
    "        ])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    baseline = HeuristicBaseline\n",
    "    project.add_step(\n",
    "        step=EvaluateBaseline(baseline),\n",
    "        checks=[CheckScoreExists(metric=METRICS[i])\n",
    "                for i in range(len(METRICS))],\n",
    "    )\n",
    "\n",
    "    model = YelpReviewsModel\n",
    "\n",
    "    EXPECTED_LOSS = - math.log(1 / NUM_CLASSES)\n",
    "    EXPECTED_LOSS\n",
    "    project.add_step(\n",
    "        CheckLossOnInit(model),\n",
    "        [CheckScoreCloseTo(metric=METRICS[3],\n",
    "                           value=EXPECTED_LOSS, rel_tol=0.1)]\n",
    "    )\n",
    "\n",
    "    project.add_step(\n",
    "        step=OverfitOneBatch(model, number_of_steps=40),\n",
    "        checks=[CheckScoreLessThan(metric=METRICS[3], value=0.05)],\n",
    "    )\n",
    "\n",
    "    project.add_step(\n",
    "        step=Overfit(model, max_epochs=50),\n",
    "        checks=[CheckScoreLessThan(metric=METRICS[3], value=0.1)],\n",
    "    )\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Hints for performing full transfer learning</summary>\n",
    "\n",
    "* To register the step call the `add_step` function with `step=EvaluateBaseline` from `art.steps` and `checks=[CheckResultExists(metric) for metric in METRICS]`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Final main()<b></summary>\n",
    "\n",
    "```py\n",
    "    data = YelpReviews()\n",
    "    project = ArtProject(\"yelpreviews\", data)\n",
    "\n",
    "    project.add_step(\n",
    "        TextDataAnalysis(),\n",
    "        [\n",
    "            CheckResultExists(\"number_of_classes\"),\n",
    "            CheckResultExists(\"class_names\"),\n",
    "            CheckResultExists(\"number_of_reviews_in_each_class\"),\n",
    "            CheckResultExists(\"number_of_unique_words\")\n",
    "        ])\n",
    "    NUM_CLASSES = 5\n",
    "    METRICS = [\n",
    "        Accuracy(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Precision(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        Recall(num_classes=NUM_CLASSES, average='macro', task='multiclass'),\n",
    "        nn.CrossEntropyLoss()\n",
    "    ]\n",
    "    project.register_metrics(METRICS)\n",
    "\n",
    "    baseline = HeuristicBaseline\n",
    "    project.add_step(\n",
    "        step=EvaluateBaseline(baseline),\n",
    "        checks=[CheckScoreExists(metric=METRICS[i])\n",
    "                for i in range(len(METRICS))],\n",
    "    )\n",
    "\n",
    "    model = YelpReviewsModel\n",
    "\n",
    "    EXPECTED_LOSS = - math.log(1 / NUM_CLASSES)\n",
    "    EXPECTED_LOSS\n",
    "    project.add_step(\n",
    "        CheckLossOnInit(model),\n",
    "        [CheckScoreCloseTo(metric=METRICS[3],\n",
    "                           value=EXPECTED_LOSS, rel_tol=0.1)]\n",
    "    )\n",
    "\n",
    "    project.add_step(\n",
    "        step=OverfitOneBatch(model, number_of_steps=40),\n",
    "        checks=[CheckScoreLessThan(metric=METRICS[3], value=0.05)],\n",
    "    )\n",
    "\n",
    "    project.add_step(\n",
    "        step=Overfit(model, max_epochs=50),\n",
    "        checks=[CheckScoreLessThan(metric=METRICS[3], value=0.1)],\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping('CrossEntropyLoss-validate', patience=6)\n",
    "    project.add_step(TransferLearning(model,\n",
    "                                      freezed_trainer_kwargs={\"max_epochs\": 3,\n",
    "                                                              \"check_val_every_n_epoch\": 2,\n",
    "                                                              \"callbacks\": [early_stopping]},\n",
    "                                      unfreezed_trainer_kwargs={\"max_epochs\": 3,\n",
    "                                                                \"check_val_every_n_epoch\": 2,\n",
    "                                                                \"callbacks\": [early_stopping]},\n",
    "                                      freeze_names=[\"bert\"],\n",
    "                                      logger=logger\n",
    "                                      ),\n",
    "                     [CheckScoreGreaterThan(metric=METRICS[0], value=0.80)])\n",
    "\n",
    "    project.run_all()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!! You've just performed transfer learning on the Yelp Reviews dataset! You can check the results in the checkpoints folder. You should see, that the model is able to achieve ~45% accuracy on the test set. It's not a lot, but it's a good result for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra tasks**\n",
    "* Add logger, currently we support Neptune and Wandb. You can initialize the loggers and pass it to every step by `logger` argument.\n",
    "* Experiment with other Checks\n",
    "* Write your own Step (and Check if needed) to perform further research\n",
    "\n",
    "<details>\n",
    "<summary> Loggers usage </summary>\n",
    "```py\n",
    "from art.loggers import NeptuneLoggerAdapter, WandbLoggerAdapter\n",
    "logger = NeptuneLoggerAdapter(\n",
    "        project=\"your_project_name_in_Neptune\")\n",
    "logger = WandbLoggerAdapter(\n",
    "        project=\"your_project_name_in_Neptune\")\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

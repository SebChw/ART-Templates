{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART tutorial for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **ART** tutorial for the MNIST dataset. In this tutorial, we will demonstrate how to perform a deep learning experiment using ART.\n",
    "\n",
    "The tutorial is divided into the following sections:\n",
    "1. Introduction to ART\n",
    "2. Installation of required packages\n",
    "3. Basic usage of ART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART is designed to support deep learning researchers providing a special pipeline to perform all experments in a unified way. Each experiment is defined of the following components:\n",
    "- **Dataset**: Each experiment is supposed to deal with one dataset. ART supports every LightningDataModule from PyTorch Lightning, and every torch Dataset with its own dataloader.\n",
    "- **Experiment**: `TODO`\n",
    "- **Metrics**: `TODO`\n",
    "- **Model**: While exploring different models, ART provides a unified way to define and train models. ART uses **ArtModule** that inherits from PyTorch Lightning's LightningModule. ArtModules are designed to be easily configurable and to support different model architectures.\n",
    "- **Step**: `TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we assume that you have already installed the ART package. If not, please refer to the [ART README](https://github.com/SebChw/art#readme)\n",
    "\n",
    "We will also need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers albumentations torchvision einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic usage of ART\n",
    "Normally after cloning the project template you would have to implement every code template inside `exp1` on your own. We would just provide the project skeleton with all classes and functions defined, but not implemented. For this tutorial, we implemented all the functions so that you can observe how to use art.\n",
    "\n",
    "Let's tackle this strong enemy with the art framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "First, we need to download the data. We highly recommend using Huggingface datasets wrapped into the Lightning Data Module. You can also use Torch Datasets and DataLoaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from art_data import MNISTDataModule\n",
    "from art.enums import INPUT, TARGET\n",
    "\n",
    "\n",
    "def get_data_module(n_train=200):\n",
    "    mnist_data = datasets.load_dataset(\"mnist\")\n",
    "\n",
    "    mnist_data = mnist_data.rename_columns({\"image\": INPUT, \"label\": TARGET})\n",
    "    mnist_data['train'] = mnist_data['train'].select(range(n_train))\n",
    "\n",
    "    return MNISTDataModule(mnist_data)\n",
    "\n",
    "\n",
    "mnist_data_module = get_data_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "\n",
    "In every project we encounter you to start with a deep look into the data. Currently, we are working on a tool that will help you with this task. For now, we left this part for you to implement on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analisys code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & experiment definition\n",
    "\n",
    "1. We define the **Experiment** - the heart of art. Later we will add steps that will be performed for this particular experiment.\n",
    "2. We register some metrics in **MetricCalculator** - this class performs metric calculations for all models defined by you during every step. The only need you must provide is a consistent format of returned data from different models. You can easily add exceptions - we don't want to calculate the loss for the Evaluate Baseline Step. If some metrics are expensive to calculate you add an exception and it won't be calculated. You don't need to modify the code to modify metrics behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.metric_calculator import MetricCalculator\n",
    "from torchmetrics import Accuracy\n",
    "import torch.nn as nn\n",
    "from art.step.steps import EvaluateBaselines\n",
    "from art.experiment.Experiment import Experiment\n",
    "\n",
    "exp = Experiment(\"exp1\")\n",
    "\n",
    "MetricCalculator.register_metric(Accuracy(task=\"multiclass\", num_classes = 10))\n",
    "MetricCalculator.register_metric(nn.CrossEntropyLoss(), exception_steps = [EvaluateBaselines])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "\n",
    "Starting any Machine Learning project it is good to have some baseline for comparison. Here in `ART` we try to encourage you to try at least one of those:\n",
    "* ML Baseline - Not for all problems we need Deep Learning. For tabular data or time series, there are ML models that you can take off the shelf and obtain very good results. \n",
    "* Heuristic Baseline - Sometimes you may have some intuition on how should be data regressed or classified. It is always worth to check how good these are.\n",
    "* Existing solution baseline - If the problem is already solved, checking existing solutions and pointing out their drawbacks to improve on is always worth doing.\n",
    "\n",
    "\n",
    "Another good habit is to separate your code into many smaller modules and don't do everything inside one gigantic Jupyter notebook. Look inside the `baselines.py` file where we have prepared everything for you.\n",
    "\n",
    "Have you observed plenty of small functions inside each of the classes? These will create pipelines of your ArtModule.\n",
    "\n",
    "E.g. `MlBaseline` has functions `parse_data_for_ml` and `baseline_train` that both create `ml_train_pipeline` that is defined inside art. As you become more familiar with `ART` you will be able to create your own pipelines and their building blocks.\n",
    "\n",
    "Finally, to offer you a high level of flexibility we pass data between all stages with the use of a dictionary (This will be changed into a better structure in the future).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines import MlBaseline, HeuristicBaseline, AlreadyExistingSolutionBaseline\n",
    "from art.step.checks import CheckScoreExists\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "baselines = [HeuristicBaseline(), MlBaseline(\n",
    "    model=LogisticRegression()), AlreadyExistingSolutionBaseline()]\n",
    "baseline_names = [baseline.name for baseline in baselines]\n",
    "exp.add_step(\n",
    "    EvaluateBaselines(\n",
    "        baselines, mnist_data_module\n",
    "    ), [CheckScoreExists('Evaluate Baselines', f\"check_{name}\", name) for name in baseline_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined our 3 baselines. You can check how these are structured in `baselines.py` file. Next we create our first step that will consist of their evaluation. We always add new steps in a form: `step to be performd`, `list_of_check_to_be_performed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, a lot has happened. But in the end, we have validated three baselines successfully. We can see, that the already existing solution is a very strong competitor. If results obtained by these baselines are satisfactory. \n",
    "\n",
    "Please visit the checkpoints folder - results of this stage are saved here. `In version 2.0 you will be able to see all the results from the dashboard`.\n",
    "\n",
    "Having baselines we can move on and try to beat them. We will use CNN for this purpose which is defined inside `models.py`. The next step is to validate loss after network initialization. If it is off, we probably messed something up in our pipeline. We have 10 classes, so on average with random initialization loss should be -log(1/10) = 2.31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MNISTModel\n",
    "from art.step.steps import CheckLossOnInit\n",
    "from art.step.checks import CheckScoreCloseTo\n",
    "MetricCalculator.to(\"cpu\")\n",
    "my_model = MNISTModel()\n",
    "\n",
    "exp.add_step(CheckLossOnInit(my_model, mnist_data_module), [CheckScoreCloseTo(\"Check Loss on Init\", \"check_loss\", \"CrossEntropyLoss-MNISTModel-TRAIN-Check Loss On Init\", 2.3, rel_tol=0.1)]) \n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are, something must be wrong in our pipeline - loss is off by big amount. Observe, two things:\n",
    "1. We didn't perform baselines evaluations again this step is already passed\n",
    "2. We added a check to our step. We want our loss to be close to 2.3 we failed to provide this result so we obtained an exception. We can't go further until we pass this stage.\n",
    "\n",
    "Let's discover another feature of art that helps you to debug the code and find problems with your pipeline - Decorators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: make an introduction what is `set visualization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "from art.visualization_decorators import set_visualization\n",
    "import logging\n",
    "lt.monkey_patch()\n",
    "\n",
    "logging.basicConfig(filename='debug_art.log', encoding='utf-8', level=logging.WARNING)\n",
    "def first_logging_func(data):\n",
    "    logging.warning(data)\n",
    "\n",
    "\n",
    "set_visualization([(my_model, \"predict\")], first_logging_func)\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may decorate any function in your pipeline with any logging function. Here it is a simple log, but in practice, you can do much, much more, which you will see later. Please visit the `debug_art.log` file. \n",
    "\n",
    "Can you spot what is wrong? Yes! We do not normalize images to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.steps[1].model.normalize_img = True\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it! We gained a little more confidence in our pipeline. Let's move further. The next step is to overfit just a single batch. If you can't do this there is for sure something wrong. From now procedure for everything else is very similar. So we import the next step and add it to our experiment with the appropriate check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another step - overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.step.steps import OverfitOneBatch\n",
    "from art.step.checks import CheckScoreLessThan\n",
    "\n",
    "overfit_batch_step = OverfitOneBatch(MNISTModel(), mnist_data_module)\n",
    "exp.add_step(overfit_batch_step, [CheckScoreLessThan(\n",
    "    \"Overfit One Batch\", \"check_loss\", \"loss_at_the_end\", 0.01)])\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no... we can't overfit. Is this an issue with the model? Maybe but at first let's see another powerful use case of art. We will debug the prediction dynamics to see how our model is evolving. We will calculate the average logit value for all images that should be classified as 1. For this purpose we create a class with memory and make it callable, later we decorate our model prediction function and casually run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionSaver:\n",
    "    def __init__(self):\n",
    "        self.logits = []\n",
    "        self.time = 0\n",
    "\n",
    "    def __call__(self, data):\n",
    "        targets = data['target'] == 1\n",
    "        logits = data['prediction']\n",
    "        \n",
    "        wanted_logits = logits[targets].mean(dim=0)\n",
    "        for i, logit in enumerate(wanted_logits):\n",
    "            self.logits.append({\n",
    "                \"time\": self.time,\n",
    "                \"logit\": logit.item(),\n",
    "                \"class\": i\n",
    "            })\n",
    "\n",
    "        self.time += 1    \n",
    "\n",
    "    def visualize(self):\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = pd.DataFrame(self.logits)\n",
    "        df = df.pivot(index='time', columns='class', values='logit')\n",
    "        df.plot()\n",
    "        plt.show()\n",
    "\n",
    "my_model = MNISTModel()\n",
    "evolution_saver = EvolutionSaver()\n",
    "set_visualization([(my_model, \"predict\")], None, evolution_saver)\n",
    "overfit_batch_step.model = my_model # TODO THIS IS DONE TO RESET WEIGHTS (THIS SHOULD BE SOMEHOW DONE IN A BETTER WAY)\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment fails, we know that. But what about the visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_saver.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see we are moving in the correct direction. 1 has the biggest logist very soon but the difference is not big enough yet - Probably we should just wait a little bit longer. Moreover, we see how powerful decorators can be. This helps us make our code more modular and fulfill the Open-Closed principle.\n",
    "\n",
    "We can solve our problem in two ways:\n",
    "* Either increase the learning rate.\n",
    "* Or train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MNISTModel()\n",
    "evolution_saver = EvolutionSaver()\n",
    "set_visualization([(my_model, \"predict\")], None, evolution_saver)\n",
    "overfit_batch_step.model = my_model\n",
    "overfit_batch_step.number_of_steps = 1000\n",
    "exp.run_all()\n",
    "evolution_saver.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! At overfitting process we'd recommend not to play with learning rate and stick with default value. It is better to just wait for a little bit longer. Now another Step can be taken. Overfitting entire training set. This is also very important as this somehow approximates our best possible score that can be achieved. If it is much below our expected result this again may indicate problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.step.steps import Overfit\n",
    "from art.step.checks import CheckScoreGreaterThan\n",
    "\n",
    "my_model = MNISTModel()\n",
    "exp = Experiment(\"exp1\")\n",
    "exp.add_step(Overfit(my_model, mnist_data_module, max_epochs=50), [CheckScoreGreaterThan(\n",
    "    \"Overfit\", \"check_loss\", \"MulticlassAccuracy-MNISTModel-TRAIN-Overfit\", 0.90)])\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, We were able to pass the next stage in the first try. Hopefully you see another challenge that we have to face. We are able to obtain nice accuracy on the training stage. But on the validation it decreases by very much. We are overfitting the training set. For this tutorial training set was chosen to be very small so this problem was expected. But now let's try to pass the finals step of this tutorial, namely regularization. When regularizing the model you should start from the most powerful regularization technique and go to weaker. Here we will use:\n",
    "\n",
    "1. getting more data - definitely one of the most powerful techniques. If possible always prefer this one.\n",
    "2. augmentations - also very strong one\n",
    "\n",
    "When applying augmentations to the images it is crucial to verify how do they influence the image. We again can use our decoration possibilities. *Under the hood `Regularize` class runs turn_on_regularization of ArtModule and the DataModule you use*.\n",
    "\n",
    "You can see what we input to the network by checking `augmentations_i.png` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from art.step.steps import Regularize\n",
    "\n",
    "class BatchSaver:\n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # We \n",
    "        if self.time < 10:\n",
    "            save_image(data['input'], f'augmentations_{self.time}.png')\n",
    "        self.time +=1\n",
    "    \n",
    "\n",
    "my_model = MNISTModel()\n",
    "mnist_data_module = get_data_module(n_train=2000) \n",
    "set_visualization([(my_model, \"predict\")], BatchSaver())\n",
    "exp.add_step(Regularize(my_model, mnist_data_module), [CheckScoreGreaterThan(\"Regularize\", \"check_loss\", \"MulticlassAccuracy-MNISTModel-VALIDATION-Regularize\", 0.80)])\n",
    "exp.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are! Hopefully, now you have incentive to try ART for your next data science project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART tutorial for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **ART** tutorial for the MNIST dataset. In this tutorial, we will demonstrate how to perform a deep learning experiment using ART.\n",
    "\n",
    "The tutorial is divided into the following sections:\n",
    "1. Introduction to ART\n",
    "2. Installation of required packages\n",
    "3. Basic usage of ART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART is designed to support deep learning researchers providing a special pipeline to perform all experments in a unified way. Each experiment is defined of the following components:\n",
    "- **Dataset**: Each Project is supposed to deal with one dataset. ART supports every LightningDataModule from PyTorch Lightning, and every torch Dataset with its own dataloader.\n",
    "- **Model**: While exploring different models, ART provides a unified way to define and train models. ART uses **ArtModule** that inherits from PyTorch Lightning's LightningModule. ArtModules are designed to be easily configurable and to support different model architectures.\n",
    "- **Step**: Unitary process that takes you closer to your final goal - good Deep Learning model. **In this tutorial we present you steps that were inspired by Andrej Karpathy's** [Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/):\n",
    "    1. EvaluateBaseline - before starting a new project it is good to know with whom we compete.\n",
    "    2. CheckLossOnInit - Checking Loss right after network initialization is a very good debug step\n",
    "    3. OverfitOneBatch - If you can't Overfit single batch, it is very unlikely that your network will work any good.\n",
    "    4. Overfit - By reducing wanted metric on training set you can observe `theoreticall` achievable minimum if this value doesn't satisfy you it is very unlikely it will be better on the test set.\n",
    "    5. Regularize - Usually gap between training and validation score is quite big and we need to introduce regularization techniques to achieve satisfactory validation accuracy.\n",
    "- **Metrics**: To make you write less code we implemented `MetricCalculator`, a special object that takes care of metric calculation between all steps. The only thing you have to do is to `register` metrics that you want to compute.\n",
    "- **Check**: For every step you must pass a list of `Check` objects that must be fulfilled for the step to be passed. You may encounter checks like `CheckScoreExists` or `CheckCloseTo`. Every check takes at least 3 arguments:\n",
    "  - Metric, which value at the end of a step will be checked. This may be e.g `nn.CrossEntropyLoss` object.\n",
    "  - Stage, During different steps you'll be interested in performance at either Training or Validation. You must pass information about this too.\n",
    "  - value, wanted value of the check\n",
    "- **Art Project**: Every project consists of many steps. You can add them to `ArtProject` which is responsible for running them. `ArtProject` also saves metadata about your steps that you can later see in `Dasboard`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we assume that you have already installed the ART package. If not, please refer to the [ART README](https://github.com/SebChw/art#readme)\n",
    "\n",
    "We will also need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T12:52:35.581409Z",
     "start_time": "2023-10-07T12:52:31.575433Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers albumentations torchvision einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic usage of ART\n",
    "Normally after cloning the project template you would have to implement every code template inside `exp1` on your own. We would just provide the project skeleton with all classes and functions defined, but not implemented. For this tutorial, we implemented all the functions so that you can observe how to use art.\n",
    "\n",
    "Let's tackle this strong enemy with the art framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "First, we need to download the data. We highly recommend using Huggingface datasets wrapped into the Lightning Data Module. You can also use Torch Datasets and DataLoaders. This time we prepared `MNISTDataModoule` for you. But if you work with your custom data you must implement it by yourself. Look inside `art_data.py` to see how we've done this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:12.903620Z",
     "start_time": "2023-10-07T14:10:12.903308Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:18.882911Z",
     "start_time": "2023-10-07T14:10:15.455111Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lightning import  seed_everything\n",
    "seed_everything(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:22.027986Z",
     "start_time": "2023-10-07T14:10:18.868432Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from art_data import MNISTDataModule\n",
    "from art.utils.enums import INPUT, TARGET\n",
    "\n",
    "def get_data_module(n_train=200):\n",
    "    mnist_data = datasets.load_dataset(\"mnist\")\n",
    "\n",
    "    mnist_data = mnist_data.rename_columns({\"image\": INPUT, \"label\": TARGET})\n",
    "    mnist_data['train'] = mnist_data['train'].select(range(n_train))\n",
    "\n",
    "    return MNISTDataModule(mnist_data)\n",
    "\n",
    "\n",
    "mnist_data_module = get_data_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "\n",
    "In every project we encourage you to start with a deep look into the data. Currently, we are working on a tool that will help you with this task. For now, we left this part for you to implement on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:22.064256Z",
     "start_time": "2023-10-07T14:10:22.029098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data analisys code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & experiment definition\n",
    "\n",
    "1. We define the **Experiment** - the heart of art. Later we will add steps that will be performed for this particular experiment.\n",
    "2. We register some metrics in **MetricCalculator** - this class performs metric calculations for all models defined by you during every step. The only need you must provide is a consistent format of returned data from different models. You can easily add exceptions - we don't want to calculate the loss for the Evaluate Baseline Step. If some metrics are expensive to calculate you add an exception and it won't be calculated. You don't need to modify the code to modify metrics behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:22.165036Z",
     "start_time": "2023-10-07T14:10:22.067936Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import torch.nn as nn\n",
    "from art.step.steps import EvaluateBaseline\n",
    "from art.experiment.Experiment import ArtProject\n",
    "\n",
    "project = ArtProject(\"exp1\", mnist_data_module)\n",
    "\n",
    "NUM_CLASSES = len(mnist_data_module.dataset['train']['target'].unique())\n",
    "accuracy_metric, ce_loss = Accuracy(task=\"multiclass\", num_classes = NUM_CLASSES), nn.CrossEntropyLoss()\n",
    "project.register_metrics([accuracy_metric, ce_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "\n",
    "Starting any Machine Learning project it is good to have some baseline for comparison. Here in `ART` we try to encourage you to try at least one of those:\n",
    "* ML Baseline - Not for all problems we need Deep Learning. For tabular data or time series, there are ML models that you can take off the shelf and obtain very good results. \n",
    "* Heuristic Baseline - Sometimes you may have some intuition on how should be data regressed or classified. It is always worth to check how good these are.\n",
    "* Existing solution baseline - If the problem is already solved, checking existing solutions and pointing out their drawbacks to improve on is always worth doing.\n",
    "\n",
    "\n",
    "Another good habit is to separate your code into many smaller modules and don't do everything inside one gigantic Jupyter notebook. Look inside the `baselines.py` file where we have prepared everything for you.\n",
    "\n",
    "Have you observed plenty of small functions inside each of the classes? These will create pipelines of your ArtModule.\n",
    "\n",
    "E.g. `MlBaseline` has functions `parse_data_for_ml` and `baseline_train` that both create `ml_train_pipeline` that is defined inside art. As you become more familiar with `ART` you will be able to create your own pipelines and their building blocks.\n",
    "\n",
    "Finally, to offer you a high level of flexibility we pass data between all stages with the use of a dictionary (This will be changed into a better structure in the future).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:23.256689Z",
     "start_time": "2023-10-07T14:10:22.159283Z"
    }
   },
   "outputs": [],
   "source": [
    "from art.core.MetricCalculator import SkippedMetric\n",
    "from baselines import MlBaseline, HeuristicBaseline, AlreadyExistingSolutionBaseline\n",
    "from art.step.checks import CheckScoreExists\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from art.utils.enums import TrainingStage\n",
    "baselines = [HeuristicBaseline(), MlBaseline(\n",
    "    model=LogisticRegression()), \n",
    "    AlreadyExistingSolutionBaseline()\n",
    "    ]\n",
    "for baseline in baselines:\n",
    "    project.add_step(\n",
    "        step = EvaluateBaseline(baseline), \n",
    "        checks = [CheckScoreExists(metric=accuracy_metric)],\n",
    "        skipped_metrics=[SkippedMetric(metric=ce_loss)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined our 3 baselines. You can check how these are structured in `baselines.py` file. Next we create our first step that will consist of their evaluation. We always add new steps in a form: `step to be performd`, `list_of_check_to_be_performed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:33.620522Z",
     "start_time": "2023-10-07T14:10:24.279221Z"
    }
   },
   "outputs": [],
   "source": [
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, a lot has happened. But in the end, we have validated three baselines successfully. We can see, that the already existing solution is a very strong competitor. If results obtained by these baselines are satisfactory. \n",
    "\n",
    "Please visit the checkpoints folder - results of this stage are saved here. `In version 2.0 you will be able to see all the results from the dashboard`.\n",
    "\n",
    "Having baselines we can move on and try to beat them. We will use CNN for this purpose which is defined inside `models.py`. The next step is to validate loss after network initialization. If it is off, we probably messed something up in our pipeline. We have 10 classes, so on average with random initialization loss should be -log(1/10) = 2.31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding CheckLossOnInitStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:36.327862Z",
     "start_time": "2023-10-07T14:10:36.290460Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "EXPECTED_LOSS = - math.log(1 / NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:07:46.908847Z",
     "start_time": "2023-10-07T14:07:46.662887Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import MNISTModel\n",
    "from art.step.steps import CheckLossOnInit\n",
    "from art.step.checks import CheckScoreCloseTo\n",
    "project.to(\"cuda\")\n",
    "my_model = MNISTModel()\n",
    "\n",
    "project.add_step(\n",
    "    CheckLossOnInit(my_model),\n",
    "    [CheckScoreCloseTo(metric=ce_loss, value=2.3, rel_tol=0.1)]) \n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are, something must be wrong in our pipeline - loss is off by big amount. Observe, two things:\n",
    "1. We didn't perform baselines evaluations again this step is already passed\n",
    "2. We added a check to our step. We want our loss to be close to 2.3 we failed to provide this result so we obtained an exception. We can't go further until we pass this stage.\n",
    "\n",
    "Let's discover another feature of art that helps you to debug the code and find problems with your pipeline - Decorators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: make an introduction what is `set visualization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:07:55.837236Z",
     "start_time": "2023-10-07T14:07:55.744476Z"
    }
   },
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "from art.core.visualizer.visualization_decorators import set_visualization\n",
    "import logging\n",
    "lt.monkey_patch()\n",
    "\n",
    "logging.basicConfig(filename='debug_art.log', level=logging.WARNING)\n",
    "def first_logging_func(data):\n",
    "    logging.warning(data)\n",
    "\n",
    "\n",
    "set_visualization([(my_model, \"predict\")], first_logging_func)\n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may decorate any function in your pipeline with any logging function. Here it is a simple log, but in practice, you can do much, much more, which you will see later. Please visit the `debug_art.log` file. \n",
    "\n",
    "Can you spot what is wrong? Yes! We do not normalize images to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:10:51.743746Z",
     "start_time": "2023-10-07T14:10:51.539254Z"
    }
   },
   "outputs": [],
   "source": [
    "new_model = MNISTModel(normalize_img=True) \n",
    "project.replace_step(CheckLossOnInit(new_model))\n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it! We gained a little more confidence in our pipeline. Let's move further. The next step is to overfit just a single batch. If you can't do this there is for sure something wrong. From now procedure for everything else is very similar. So we import the next step and add it to our experiment with the appropriate check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another step - overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:08:02.523612Z",
     "start_time": "2023-10-07T14:08:02.428466Z"
    }
   },
   "outputs": [],
   "source": [
    "from art.step.steps import OverfitOneBatch\n",
    "from art.step.checks import CheckScoreLessThan\n",
    "\n",
    "project.add_step(OverfitOneBatch(MNISTModel()),\n",
    "            [CheckScoreLessThan(metric=ce_loss, value=0.01)])\n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no... we can't overfit. Is this an issue with the model? Maybe but at first let's see another powerful use case of art. We will debug the prediction dynamics to see how our model is evolving. We will calculate the average logit value for all images that should be classified as 1. For this purpose we create a class with memory and make it callable, later we decorate our model prediction function and casually run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:08:04.760395Z",
     "start_time": "2023-10-07T14:08:04.628754Z"
    }
   },
   "outputs": [],
   "source": [
    "class EvolutionSaver:\n",
    "    def __init__(self):\n",
    "        self.logits = []\n",
    "        self.time = 0\n",
    "\n",
    "    def __call__(self, data):\n",
    "        targets = data['target'] == 1\n",
    "        logits = data['prediction']\n",
    "        \n",
    "        wanted_logits = logits[targets].mean(dim=0)\n",
    "        for i, logit in enumerate(wanted_logits):\n",
    "            self.logits.append({\n",
    "                \"time\": self.time,\n",
    "                \"logit\": logit.item(),\n",
    "                \"class\": i\n",
    "            })\n",
    "\n",
    "        self.time += 1    \n",
    "\n",
    "    def visualize(self):\n",
    "        import pandas as pd\n",
    "        if len(self.logits) == 0:\n",
    "            print(\"Step was not run and logits are empty\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.logits)\n",
    "        df = df.pivot(index='time', columns='class', values='logit')\n",
    "        return df.plot(xlabel=\"epoch number\", ylabel=\"logit value\", title=\"Evolution of logits values when digit 1 is a target\")\n",
    "\n",
    "my_model = MNISTModel()\n",
    "evolution_saver = EvolutionSaver()\n",
    "set_visualization([(my_model, \"predict\")], None, evolution_saver)\n",
    "project.replace_step(OverfitOneBatch(my_model))\n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment fails, we know that. But what about the visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:11:13.327527Z",
     "start_time": "2023-10-07T14:11:12.894425Z"
    }
   },
   "outputs": [],
   "source": [
    "plot = evolution_saver.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how about saving this plot to a logging service like Neptune?\n",
    "\n",
    "Here comes yet another help from art called logger adapter.\n",
    "Thanks to this, you can always get back to the visualization you have created at any time - even if you use an external computing unit.\n",
    "\n",
    "Create a Neptune project, paste your api_token and project name, and use the logger as easy as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:11:24.071828Z",
     "start_time": "2023-10-07T14:11:20.448301Z"
    }
   },
   "outputs": [],
   "source": [
    "from art.utils.LoggerAdapter import NeptuneLoggerAdapter\n",
    "from neptune.exceptions import NeptuneMissingApiTokenException\n",
    "\n",
    "project_name = input(\"Provide neptune project name\")\n",
    "experiment_name = \"art-overfitting-one-batch\"\n",
    "try:\n",
    "    logger = NeptuneLoggerAdapter(project=project_name, custom_run_id=experiment_name)\n",
    "    if plot is not None:\n",
    "        logger.log_figure(plot.get_figure(), \"logit_evolution\")\n",
    "except NeptuneMissingApiTokenException as e:\n",
    "    print(\"To enable logging, please set your NEPTUNE_API_TOKEN environment variable and reset the notebook, or pass your token and project name directly to NeptuneLoggerAdapter like:\\n    NeptuneLoggerAdapter(api_token=your_token, project=project_name)\\n\")\n",
    "    print(\"For more information, please visit https://docs.neptune.ai/setup/setting_api_token/.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to debugging.\n",
    "\n",
    "So, as you can see we are moving in the correct direction. 1 has the biggest logist very soon but the difference is not big enough yet - Probably we should just wait a little bit longer. Moreover, we see how powerful decorators can be. This helps us make our code more modular and fulfill the Open-Closed principle.\n",
    "\n",
    "We can solve our problem in two ways:\n",
    "* Either increase the learning rate.\n",
    "* Or train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:11:40.042831Z",
     "start_time": "2023-10-07T14:11:24.074849Z"
    }
   },
   "outputs": [],
   "source": [
    "my_model = MNISTModel()\n",
    "evolution_saver = EvolutionSaver()\n",
    "set_visualization([(my_model, \"predict\")], None, evolution_saver)\n",
    "project.replace_step(OverfitOneBatch(my_model, number_of_steps=500))\n",
    "project.run_all()\n",
    "plot = evolution_saver.visualize()\n",
    "if plot is not None:\n",
    "    logger.log_figure(plot.get_figure(), \"logit_evolution2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:11:40.641790Z",
     "start_time": "2023-10-07T14:11:40.044026Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! At overfitting process we'd recommend not to play with learning rate and stick with default value. It is better to just wait for a little bit longer. Now another Step can be taken. Overfitting entire training set. This is also very important as this somehow approximates our best possible score that can be achieved. If it is much below our expected result this again may indicate problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:08:33.806046Z",
     "start_time": "2023-10-07T14:08:33.334167Z"
    }
   },
   "outputs": [],
   "source": [
    "from art.step.steps import Overfit\n",
    "from art.step.checks import CheckScoreGreaterThan\n",
    "\n",
    "experiment_name = \"overfitting-train-set\"\n",
    "logger = NeptuneLoggerAdapter(project=project_name, custom_run_id=experiment_name)\n",
    "\n",
    "my_model = MNISTModel()\n",
    "project.add_step(Overfit(my_model, max_epochs=50, logger=logger), \n",
    "            [CheckScoreGreaterThan(metric=accuracy_metric, value=0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:08:36.552606Z",
     "start_time": "2023-10-07T14:08:36.503657Z"
    }
   },
   "outputs": [],
   "source": [
    "project.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T14:08:42.455047Z",
     "start_time": "2023-10-07T14:08:41.404186Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, We were able to pass the next stage in the first try. Hopefully you see another challenge that we have to face. We are able to obtain nice accuracy on the training stage. But on the validation it decreases by very much. We are overfitting the training set. For this tutorial training set was chosen to be very small so this problem was expected. But now let's try to pass the finals step of this tutorial, namely regularization. When regularizing the model you should start from the most powerful regularization technique and go to weaker. Here we will use:\n",
    "\n",
    "1. getting more data - definitely one of the most powerful techniques. If possible always prefer this one.\n",
    "2. augmentations - also very strong one\n",
    "\n",
    "When applying augmentations to the images it is crucial to verify how do they influence the image. We again can use our decoration possibilities. *Under the hood `Regularize` class runs turn_on_regularization of ArtModule and the DataModule you use*.\n",
    "\n",
    "You can see what we input to the network by checking `augmentations_i.png` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from art.step.steps import Regularize\n",
    "\n",
    "class BatchSaver:\n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if self.time < 10:\n",
    "            save_image(data['input'], f'augmentations_{self.time}.png')\n",
    "        self.time +=1\n",
    "\n",
    "experiment_name = \"regularization\"\n",
    "logger = NeptuneLoggerAdapter(project=project_name, custom_run_id=experiment_name)\n",
    "\n",
    "my_model = MNISTModel()\n",
    "mnist_data_module = get_data_module(n_train=2000)\n",
    "project.update_datamodule(mnist_data_module)\n",
    "set_visualization([(my_model, \"predict\")], BatchSaver())\n",
    "project.add_step(Regularize(my_model, logger=logger, trainer_kwargs={\"max_epochs\":50, \"check_val_every_n_epoch\":50}), \n",
    "        [CheckScoreGreaterThan(metric=accuracy_metric, value=0.80)])\n",
    "\n",
    "project.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T13:48:59.707383Z",
     "start_time": "2023-10-07T13:48:59.563664Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are! Hopefully, now you have incentive to try ART for your next data science project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
